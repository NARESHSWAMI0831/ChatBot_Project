# ü§ñ AI Chat + OCR Assistant

[![Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/downloads/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.68.0%2B-green.svg)](https://fastapi.tiangolo.com)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.15.0%2B-red.svg)](https://streamlit.io)
[![License](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://github.com/username/AI_Chat_OCR_Project/graphs/commit-activity)

A powerful and versatile application that combines intelligent chat capabilities with advanced document processing. Built with Ollama, FastAPI, and Streamlit, this platform enables seamless conversations while extracting and analyzing text from various document formats.


## üåü Overview

This project brings together state-of-the-art technologies to create an intelligent document processing and chat platform:

- **AI-Powered Conversations**: Leverages Ollama for natural and context-aware responses
- **Advanced OCR**: Dual-engine approach for maximum text extraction accuracy
- **Modern Web Interface**: Responsive and intuitive design using Streamlit
- **Robust Backend**: FastAPI-powered backend for reliable performance
- **Multi-Format Support**: Process various document types in one place

## ‚ú® Features

- üí¨ **Smart Chat Interface**
  - Real-time AI conversations
  - Persistent chat history
  - Dynamic session management
  - Automatic chat naming

- üìë **Document Processing**
  - Images (JPG, PNG)
  - PDF documents
  - Word files (.docx)
  - AI-powered text summarization

- üõ†Ô∏è **Technical Stack**
  - Frontend: Streamlit
  - Backend: FastAPI
  - AI: Ollama
  - OCR: EasyOCR + Tesseract
  - Real-time processing

## üöÄ Quick Start

### Prerequisites
- Python 3.8+
- Tesseract-OCR (Windows users)
- Ollama

### Setup Steps

1. **Install Requirements**
   ```bash
   pip install -r requirements.txt
   ```

2. **Start Ollama**
   ```bash
   ollama serve
   ollama pull llama3
   ```

3. **Launch Backend**
   ```bash
   uvicorn backend:app --reload
   ```

4. **Start Frontend**
   ```bash
   streamlit run app.py
   ```

Visit `http://localhost:8501` to access the application.

## üí° Key Features

- **Smart Document Processing**: Extract text from images, PDFs, and Word documents
- **Dual OCR Engine**: Combines EasyOCR and Tesseract for improved accuracy
- **AI-Powered Summaries**: Automatic summarization of extracted content
- **Interactive Chat**: Natural conversations with AI
- **Session Management**: Organize and maintain multiple chat sessions
- **Real-time Processing**: Instant feedback and responses

## üîß Configuration

### Environment Variables
- `OLLAMA_URL`: Ollama API endpoint (default: http://localhost:11434/api/generate)
- `MODEL_NAME`: AI model selection (default: llama3)

### Windows-specific Setup
For Windows users, ensure Tesseract-OCR is installed and configured:
```python
pytesseract.pytesseract.tesseract_cmd = r"C:\Program Files\Tesseract-OCR\tesseract.exe"
```

## üìñ Usage

### Chat Interface
1. **Start a Session**
   - Click "‚ûï New Chat" to begin
   - Your conversations are automatically saved
   - Switch between multiple chat sessions

2. **Document Processing**
   - Drag & drop or click to upload files
   - Supported formats:
     - Images: JPG, JPEG, PNG
     - Documents: PDF, DOCX
   - Real-time progress indicators
   - Automatic AI summarization

3. **Advanced Features**
   - **Context Awareness**: AI remembers previous conversations
   - **Smart Naming**: Chats are automatically named based on content
   - **Error Handling**: Graceful handling of processing issues
   - **Responsive Design**: Works on desktop and mobile

### API Integration

The backend provides two main endpoints:

```python
# Chat endpoint
POST /chat
{
    "message": "Your message here",
    "history": [{"role": "user", "content": "Previous messages"}]
}

# OCR endpoint
POST /ocr
- Multipart form data with file upload
- Supports: JPG, PNG, PDF, DOCX
```

## üîç Advanced Configuration

### Custom Model Settings
```python
# config.py
MODEL_SETTINGS = {
    "temperature": 0.7,
    "max_tokens": 2000,
    "top_p": 0.9
}
```

### Performance Tuning
- Adjust `BATCH_SIZE` for OCR processing
- Configure memory limits for file uploads
- Set appropriate timeouts for API calls

### Security Considerations
- Rate limiting implementation
- File type validation
- Size restrictions
- Error handling

## üîÑ Development Workflow

1. **Setup Development Environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # or venv\Scripts\activate on Windows
   pip install -r requirements-dev.txt
   ```

2. **Running Tests**
   ```bash
   pytest tests/
   pytest --cov=app tests/
   ```

3. **Code Style**
   ```bash
   flake8 .
   black .
   isort .
   ```

## ü§ù Contributing

We welcome contributions to improve the AI Chat + OCR Assistant! Here's how you can help:

### Contributing Guidelines

1. **Fork & Clone**
   ```bash
   git clone https://github.com/yourusername/AI_Chat_OCR_Project.git
   cd AI_Chat_OCR_Project
   ```

2. **Create a Branch**
   ```bash
   git checkout -b feature/amazing-feature
   ```

3. **Make Changes**
   - Follow the code style
   - Add tests if applicable
   - Update documentation

4. **Submit PR**
   - Clear description of changes
   - Reference any related issues
   - Update README if needed

### Development Best Practices

- Write clean, documented code
- Follow PEP 8 guidelines
- Add type hints
- Include error handling
- Write unit tests

## üìä Performance

### Benchmarks
- OCR Processing: ~2-3 seconds per page
- Chat Response: ~1 second
- Memory Usage: < 500MB
- Concurrent Users: 50+

### System Requirements
- CPU: 2+ cores recommended
- RAM: 4GB minimum
- Storage: 1GB for installation
- GPU: Optional, improves OCR speed

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üì¨ Contact & Support

- Create an issue for bug reports
- Join our [Discord community](https://discord.gg/your-server)
- Email: support@yourproject.com

## üôè Acknowledgments

- Ollama team for the AI model
- EasyOCR and Tesseract communities
- FastAPI and Streamlit developers
- All our contributors

---

<div align="center">

Built with ‚ù§Ô∏è using Ollama, FastAPI, and Streamlit

[Documentation](docs/) ‚Ä¢ [Report Bug](issues) ‚Ä¢ [Request Feature](issues)

</div>
